{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5uFjrnAXs8B"
      },
      "source": [
        "# N-gram Language Models - Practice with Real Data\n",
        "\n",
        "**Course:** Natural Language Processing  \n",
        "**Topic:** Building N-gram Models with HuggingFace Datasets  \n",
        "\n",
        "In this notebook, you will:\n",
        "1. Load real text corpora from HuggingFace\n",
        "2. Implement n-gram language models from scratch\n",
        "3. Train and evaluate your models\n",
        "4. Generate text and compare different approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gerySDWlXs8F"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jznvfjAdXs8G",
        "outputId": "757d2e46-fdd1-4c81-c5f4-8d378aa78814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install datasets -q\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"✓ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Re9W_gaXs8H"
      },
      "source": [
        "## Part 1: Load Dataset from HuggingFace\n",
        "\n",
        "We'll use the **WikiText-2** dataset, which contains good quality text from Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE9kfufVXs8I",
        "outputId": "e952b115-ea71-4a16-9082-c0acbef43b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WikiText-2 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset structure:\n",
            "DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 4358\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 36718\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3760\n",
            "    })\n",
            "})\n",
            "\n",
            "Sample from training set:\n",
            "{'text': ''}\n",
            "\n",
            "============================================================\n",
            "{'text': ' = Valkyria Chronicles III = \\n'}\n"
          ]
        }
      ],
      "source": [
        "# Load WikiText-2 dataset\n",
        "print(\"Loading WikiText-2 dataset...\")\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)\n",
        "\n",
        "# Explore the data\n",
        "print(\"\\nSample from training set:\")\n",
        "print(dataset['train'][0])\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(dataset['train'][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9FFM2iZXs8I"
      },
      "source": [
        "### Exercise 1.1: Data Exploration\n",
        "\n",
        "Answer these questions:\n",
        "1. How many examples are in train/validation/test splits?\n",
        "2. What does each example look like?\n",
        "3. How many empty or very short texts are there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm5UMCEhXs8J",
        "outputId": "396e0481-ee8c-41e6-a9ee-fe729b4402af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 36718\n",
            "Validation size: 3760\n",
            "Test size: 4358\n",
            "\n",
            "First 5 examples from training set:\n",
            "Example 1: \n",
            "Example 2:  = Valkyria Chronicles III = \n",
            "\n",
            "Example 3: \n",
            "Example 4:  Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
            "\n",
            "Example 5:  The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
            "\n",
            "\n",
            "Checking for empty or short texts:\n",
            "Number of empty or short texts (less than 5 words) in training set: 13689\n"
          ]
        }
      ],
      "source": [
        "train_size = len(dataset['train'])\n",
        "valid_size = len(dataset['validation'])\n",
        "test_size = len(dataset['test'])\n",
        "\n",
        "print(f\"Train size: {train_size}\")\n",
        "print(f\"Validation size: {valid_size}\")\n",
        "print(f\"Test size: {test_size}\")\n",
        "\n",
        "# Question 2: Look at a few examples\n",
        "print(\"\\nFirst 5 examples from training set:\")\n",
        "for i in range(5):\n",
        "    print(f\"Example {i+1}: {dataset['train'][i]['text']}\")\n",
        "\n",
        "\n",
        "# Question 3: Check for empty or short texts\n",
        "print(\"\\nChecking for empty or short texts:\")\n",
        "empty_or_short_texts = 0\n",
        "for item in dataset['train']:\n",
        "    text = item['text'].strip()\n",
        "    if not text or len(text.split()) < 5:\n",
        "        empty_or_short_texts += 1\n",
        "\n",
        "print(f\"Number of empty or short texts (less than 5 words) in training set: {empty_or_short_texts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiUB8tJJXs8K"
      },
      "source": [
        "## Part 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdgUmGBXs8K"
      },
      "source": [
        "### Exercise 2.1: Implement Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJZFqrJ_Xs8L",
        "outputId": "295dd26a-7e71-4fc9-8b2b-bdfb2deccfe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hello World! This is a test.\n",
            "Expected: ['<s>', 'hello', 'world!', 'this', 'is', 'a', 'test.', '</s>']\n",
            "Your output: ['<s>', 'hello', 'world!', 'this', 'is', 'a', 'test.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text: str, add_start_end: bool = True) -> List[str]:\n",
        "    \"\"\"\n",
        "    Preprocess text:\n",
        "    - Convert to lowercase\n",
        "    - Split into tokens (simple whitespace tokenization)\n",
        "    - Add <s> and </s> tokens if requested\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        add_start_end: Whether to add start/end tokens\n",
        "\n",
        "    Returns:\n",
        "        List of tokens\n",
        "    \"\"\"\n",
        "    # Step 1: Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Split on whitespace\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Step 3: Add <s> and </s> if requested\n",
        "    if add_start_end:\n",
        "        tokens = ['<s>'] + tokens + ['</s>']\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test your function\n",
        "test_text = \"Hello World! This is a test.\"\n",
        "tokens = preprocess_text(test_text, add_start_end=True)\n",
        "print(f\"Input: {test_text}\")\n",
        "print(f\"Expected: ['<s>', 'hello', 'world!', 'this', 'is', 'a', 'test.', '</s>']\")\n",
        "print(f\"Your output: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfhHo_3lXs8L"
      },
      "source": [
        "### Exercise 2.2: Prepare Training Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef-zH28WXs8L",
        "outputId": "f8683399-7524-45ca-9a73-4365504097bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:   4%|▍         | 1538/36718 [00:00<00:01, 33375.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus size: 1000 sentences\n",
            "First sentence: ['<s>', '=', 'valkyria', 'chronicles', 'iii', '=', '</s>']...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def prepare_corpus(dataset_split, max_examples: int = None) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare corpus from dataset split.\n",
        "\n",
        "    Args:\n",
        "        dataset_split: HuggingFace dataset split\n",
        "        max_examples: Maximum number of examples to use (None = all)\n",
        "\n",
        "    Returns:\n",
        "        List of tokenized sentences\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "    count = 0\n",
        "    for item in tqdm(dataset_split, desc=\"Processing\"):\n",
        "        text = item['text'].strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Preprocess each text\n",
        "        tokens = preprocess_text(text, add_start_end=True)\n",
        "\n",
        "        # Add to corpus only if result has more than 2 tokens (s, /s)\n",
        "        if len(tokens) > 2:\n",
        "            corpus.append(tokens)\n",
        "            count += 1\n",
        "\n",
        "        # Stop at max_examples if specified\n",
        "        if max_examples and count >= max_examples:\n",
        "            break\n",
        "\n",
        "    return corpus\n",
        "\n",
        "# Prepare training corpus (use subset for speed)\n",
        "print(\"Preparing training corpus...\")\n",
        "train_corpus = prepare_corpus(dataset['train'], max_examples=1000)\n",
        "\n",
        "print(f\"\\nCorpus size: {len(train_corpus)} sentences\")\n",
        "print(f\"First sentence: {train_corpus[0][:10]}...\")  # Show first 10 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp_VFAWvXs8M"
      },
      "source": [
        "## Part 3: Build Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-mb-K6-Xs8M"
      },
      "source": [
        "### Exercise 3.1: Build Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPoZCZZPXs8M",
        "outputId": "0bf2747c-6aa0-452d-a170-ccfa61810948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10690\n",
            "Sample words: ['loaded', 'disapproved', 'genera', 'roman', 'perfectly', 'icon', 'triads', 'bag', 'large', 'masted', 'roberts', 'radetzky', 'interact', 'combat', 'dance', 'wanted', 'avalanche', 'def', 'understanding', 'brick']\n"
          ]
        }
      ],
      "source": [
        "def build_vocabulary(corpus: List[List[str]]) -> set:\n",
        "    \"\"\"\n",
        "    Build vocabulary from corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus: List of tokenized sentences\n",
        "\n",
        "    Returns:\n",
        "        Set of unique tokens\n",
        "    \"\"\"\n",
        "    # Hint: Create a set and add all tokens from all sentences\n",
        "    vocabulary = set()\n",
        "    for sentence in corpus:\n",
        "        for token in sentence:\n",
        "            vocabulary.add(token)\n",
        "    return vocabulary\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocabulary(train_corpus)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Sample words: {list(vocab)[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAWE_LciXs8M"
      },
      "source": [
        "## Part 4: Implement N-gram Counting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO4vlcVSXs8N"
      },
      "source": [
        "### Exercise 4.1: Count Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iepohNoIXs8N",
        "outputId": "5d7a0a70-5b66-4d1f-9602-3a488d854533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unigrams: 90023\n",
            "Unique unigrams: 10690\n",
            "\n",
            "Most common unigrams:\n",
            "[('the', 5757), (',', 4468), ('.', 3203), ('of', 2568), ('and', 2237), ('in', 1927), ('to', 1655), ('a', 1574), ('=', 1191), ('<s>', 1000), ('</s>', 1000), ('\"', 961), ('was', 865), ('@-@', 782), ('with', 697), ('as', 663), ('for', 662), ('that', 623), (\"'s\", 606), ('on', 594)]\n"
          ]
        }
      ],
      "source": [
        "def count_unigrams(corpus: List[List[str]]) -> Counter:\n",
        "    \"\"\"\n",
        "    Count unigram frequencies.\n",
        "\n",
        "    Args:\n",
        "        corpus: List of tokenized sentences\n",
        "\n",
        "    Returns:\n",
        "        Counter with unigram counts\n",
        "    \"\"\"\n",
        "    unigram_counts = Counter()\n",
        "\n",
        "    # TODO: Implement this function\n",
        "    # Iterate through corpus and count each token\n",
        "    for sentence in corpus:\n",
        "        unigram_counts.update(sentence)\n",
        "\n",
        "    return unigram_counts\n",
        "\n",
        "# Test\n",
        "unigram_counts = count_unigrams(train_corpus)\n",
        "print(f\"Total unigrams: {sum(unigram_counts.values())}\")\n",
        "print(f\"Unique unigrams: {len(unigram_counts)}\")\n",
        "print(f\"\\nMost common unigrams:\")\n",
        "print(unigram_counts.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5XngCsgXs8N"
      },
      "source": [
        "### Exercise 4.2: Count Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCdQOGtVXs8N",
        "outputId": "07f76cbb-f113-488b-e8b0-03177c0c5bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total bigrams: 89023\n",
            "Unique bigrams: 50573\n",
            "\n",
            "Most common bigrams:\n",
            "[(('of', 'the'), 837), (('=', '='), 656), ((',', 'and'), 607), (('.', '</s>'), 578), (('.', 'the'), 526), (('in', 'the'), 498), ((',', 'the'), 316), (('to', 'the'), 268), (('<s>', '='), 264), (('=', '</s>'), 264), (('on', 'the'), 199), (('<s>', 'the'), 181), (('for', 'the'), 178), (('and', 'the'), 177), (('with', 'the'), 170), (('at', 'the'), 157), (('.', 'in'), 156), (('.', '\"'), 130), ((',', 'a'), 127), ((',', 'which'), 124)]\n"
          ]
        }
      ],
      "source": [
        "def count_bigrams(corpus: List[List[str]]) -> Counter:\n",
        "    \"\"\"\n",
        "    Count bigram frequencies.\n",
        "\n",
        "    Args:\n",
        "        corpus: List of tokenized sentences\n",
        "\n",
        "    Returns:\n",
        "        Counter with bigram counts (bigrams as tuples)\n",
        "    \"\"\"\n",
        "    bigram_counts = Counter()\n",
        "\n",
        "    # For each sentence, create pairs of consecutive words\n",
        "    # Store as tuples: (word1, word2)\n",
        "    for sentence in corpus:\n",
        "        for i in range(len(sentence) - 1):\n",
        "            bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
        "\n",
        "    return bigram_counts\n",
        "\n",
        "# Test\n",
        "bigram_counts = count_bigrams(train_corpus)\n",
        "print(f\"Total bigrams: {sum(bigram_counts.values())}\")\n",
        "print(f\"Unique bigrams: {len(bigram_counts)}\")\n",
        "print(f\"\\nMost common bigrams:\")\n",
        "print(bigram_counts.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSmg_zYaXs8N"
      },
      "source": [
        "### Exercise 4.3: Count Trigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7mOjjrfXs8N",
        "outputId": "b6c9ad8d-b39c-49bb-f841-d9e77bf677b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trigrams: 88023\n",
            "Unique trigrams: 76398\n",
            "\n",
            "Most common trigrams:\n",
            "[(('<s>', '=', '='), 230), (('=', '=', '</s>'), 230), (('=', '=', '='), 196), ((',', 'and', 'the'), 65), (('the', 'blue', 'jackets'), 39), (('one', 'of', 'the'), 37), (('.', '\"', '</s>'), 34), ((';', 'blackie', ','), 30), (('as', 'well', 'as'), 29), (('\"', '.', '</s>'), 27), ((',', 'such', 'as'), 27), (('the', 'gold', 'dollar'), 27), (('strapping', 'young', 'lad'), 26), (('the', 'united', 'states'), 25), (('.', 'she', 'was'), 24), (('.', 'it', 'was'), 22), ((',', 'and', 'a'), 22), (('.', 'in', 'the'), 22), (('south', 'of', 'heaven'), 22), (('in', 'the', 'uk'), 21)]\n"
          ]
        }
      ],
      "source": [
        "def count_trigrams(corpus: List[List[str]]) -> Counter:\n",
        "    \"\"\"\n",
        "    Count trigram frequencies.\n",
        "\n",
        "    Args:\n",
        "        corpus: List of tokenized sentences\n",
        "\n",
        "    Returns:\n",
        "        Counter with trigram counts (trigrams as tuples)\n",
        "    \"\"\"\n",
        "    trigram_counts = Counter()\n",
        "\n",
        "    # For each sentence, create triples of consecutive words\n",
        "    # Store as tuples: (word1, word2, word3)\n",
        "    for sentence in corpus:\n",
        "        for i in range(len(sentence) - 2):\n",
        "            trigram_counts[(sentence[i], sentence[i+1], sentence[i+2])] += 1\n",
        "\n",
        "    return trigram_counts\n",
        "\n",
        "# Test\n",
        "trigram_counts = count_trigrams(train_corpus)\n",
        "print(f\"Total trigrams: {sum(trigram_counts.values())}\")\n",
        "print(f\"Unique trigrams: {len(trigram_counts)}\")\n",
        "print(f\"\\nMost common trigrams:\")\n",
        "print(trigram_counts.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv4Hjr5IXs8O"
      },
      "source": [
        "## Part 5: Implement Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYQ6jAAmXs8O"
      },
      "source": [
        "### Exercise 5.1: Unigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgUUBaAVXs8O",
        "outputId": "8b162efd-7cbd-4c47-d7da-def9182c20e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Probabilities:\n",
            "  P(the) = 0.063950\n",
            "  P(of) = 0.028526\n",
            "  P(and) = 0.024849\n",
            "  P(<s>) = 0.011108\n",
            "  P(</s>) = 0.011108\n"
          ]
        }
      ],
      "source": [
        "class UnigramModel:\n",
        "    \"\"\"Unigram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]]):\n",
        "        self.unigram_counts = count_unigrams(corpus)\n",
        "        self.total_words = sum(self.unigram_counts.values())\n",
        "\n",
        "    def probability(self, word: str) -> float:\n",
        "        \"\"\"Calculate P(word).\"\"\"\n",
        "        # P(word) = C(word) / N\n",
        "        count = self.unigram_counts.get(word, 0)\n",
        "        return count / self.total_words if self.total_words > 0 else 0.0\n",
        "\n",
        "    def log_probability(self, word: str) -> float:\n",
        "        \"\"\"Calculate log P(word).\"\"\"\n",
        "        # Handle zero probability by returning float('-inf')\n",
        "        prob = self.probability(word)\n",
        "        return np.log(prob) if prob > 0 else float('-inf')\n",
        "\n",
        "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
        "        \"\"\"Calculate log P(sentence).\"\"\"\n",
        "        # Sum of log probabilities of all words\n",
        "        log_prob_sum = 0.0\n",
        "        for word in sentence:\n",
        "            log_prob_sum += self.log_probability(word)\n",
        "        return log_prob_sum\n",
        "\n",
        "# Test your model\n",
        "unigram_model = UnigramModel(train_corpus)\n",
        "test_words = [\"the\", \"of\", \"and\", \"<s>\", \"</s>\"]\n",
        "print(\"Unigram Probabilities:\")\n",
        "for word in test_words:\n",
        "    prob = unigram_model.probability(word)\n",
        "    print(f\"  P({word}) = {prob:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnUSpfLxXs8O"
      },
      "source": [
        "### Exercise 5.2: Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovtIyxR2Xs8O",
        "outputId": "8440df53-1ed8-4b0e-ba40-e68d2db79bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probabilities:\n",
            "  P(the | <s>) = 0.181000\n",
            "  P(and | the) = 0.000000\n",
            "  P(the | of) = 0.325935\n"
          ]
        }
      ],
      "source": [
        "class BigramModel:\n",
        "    \"\"\"Bigram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]]):\n",
        "        self.unigram_counts = count_unigrams(corpus)\n",
        "        self.bigram_counts = count_bigrams(corpus)\n",
        "\n",
        "    def probability(self, word: str, previous_word: str) -> float:\n",
        "        \"\"\"Calculate P(word | previous_word).\"\"\"\n",
        "        # P(wi | wi-1) = C(wi-1, wi) / C(wi-1)\n",
        "        count_prev_word = self.unigram_counts.get(previous_word, 0)\n",
        "        if count_prev_word == 0:\n",
        "            return 0.0 # Cannot divide by zero if previous word never appeared\n",
        "\n",
        "        count_bigram = self.bigram_counts.get((previous_word, word), 0)\n",
        "        return count_bigram / count_prev_word\n",
        "\n",
        "    def log_probability(self, word: str, previous_word: str) -> float:\n",
        "        \"\"\"Calculate log P(word | previous_word).\"\"\"\n",
        "        prob = self.probability(word, previous_word)\n",
        "        return np.log(prob) if prob > 0 else float('-inf')\n",
        "\n",
        "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
        "        \"\"\"Calculate log P(sentence).\"\"\"\n",
        "        # Sum log probabilities: log P(w2|w1) + log P(w3|w2) + ...\n",
        "        # Assuming sentence starts with <s>\n",
        "        log_prob_sum = 0.0\n",
        "        if len(sentence) < 2:\n",
        "            # A sentence with 0 or 1 token (e.g., just '<s>' or '<s> </s>') might not make sense for bigrams\n",
        "            # For simplicity, we'll return -inf or a reasonable default for such cases\n",
        "            # If it's just '<s>', its probability is 1 (log_prob = 0) in the context of start token itself\n",
        "            # However, typically bigram sentences have at least <s> and another word.\n",
        "            return 0.0 # log P(<s>) is effectively 0 in this context\n",
        "\n",
        "        # First term P(w1|<s>) for sentence[1] given sentence[0] (which is <s>)\n",
        "        # The loop naturally handles this by starting from the second word\n",
        "        for i in range(1, len(sentence)): # Start from the second word (index 1)\n",
        "            log_prob_sum += self.log_probability(sentence[i], sentence[i-1])\n",
        "        return log_prob_sum\n",
        "\n",
        "# Test your model\n",
        "bigram_model = BigramModel(train_corpus)\n",
        "test_bigrams = [(\"<s>\", \"the\"), (\"the\", \"and\"), (\"of\", \"the\")]\n",
        "print(\"Bigram Probabilities:\")\n",
        "for w1, w2 in test_bigrams:\n",
        "    prob = bigram_model.probability(w2, w1)\n",
        "    print(f\"  P({w2} | {w1}) = {prob:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwGRyX9CXs8P"
      },
      "source": [
        "### Exercise 5.3: Trigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct9BGPIQXs8P",
        "outputId": "0808435d-4e37-4a87-926c-4292ed72cf16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(valkyria | <s>, =) = 0.003788\n"
          ]
        }
      ],
      "source": [
        "class TrigramModel:\n",
        "    \"\"\"Trigram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]]):\n",
        "        self.bigram_counts = count_bigrams(corpus)\n",
        "        self.trigram_counts = count_trigrams(corpus)\n",
        "\n",
        "    def probability(self, word: str, prev_word1: str, prev_word2: str) -> float:\n",
        "        \"\"\"Calculate P(word | prev_word2, prev_word1).\"\"\"\n",
        "        # P(wi | wi-2, wi-1) = C(wi-2, wi-1, wi) / C(wi-2, wi-1)\n",
        "        context_bigram_count = self.bigram_counts.get((prev_word2, prev_word1), 0)\n",
        "        if context_bigram_count == 0:\n",
        "            return 0.0 # Cannot divide by zero if context bigram never appeared\n",
        "\n",
        "        trigram_count = self.trigram_counts.get((prev_word2, prev_word1, word), 0)\n",
        "        return trigram_count / context_bigram_count\n",
        "\n",
        "    def log_probability(self, word: str, prev_word1: str, prev_word2: str) -> float:\n",
        "        \"\"\"Calculate log P(word | prev_word2, prev_word1).\"\"\"\n",
        "        prob = self.probability(word, prev_word1, prev_word2)\n",
        "        return np.log(prob) if prob > 0 else float('-inf')\n",
        "\n",
        "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
        "        \"\"\"Calculate log P(sentence).\"\"\"\n",
        "        # Sum log probabilities: log P(w3|w1,w2) + log P(w4|w2,w3) + ...\n",
        "        # Assuming sentence starts with <s>\n",
        "        log_prob_sum = 0.0\n",
        "        if len(sentence) < 3:\n",
        "            # A sentence with less than 3 tokens (e.g., '<s>', '<s> w1 </s>')\n",
        "            # For simplicity, we'll return 0.0 for such cases, as effectively the initial <s>'s have prob 1\n",
        "            return 0.0\n",
        "\n",
        "        # The loop naturally handles this by starting from the third word\n",
        "        for i in range(2, len(sentence)): # Start from the third word (index 2)\n",
        "            log_prob_sum += self.log_probability(sentence[i], sentence[i-1], sentence[i-2])\n",
        "        return log_prob_sum\n",
        "\n",
        "# Test your model\n",
        "trigram_model = TrigramModel(train_corpus)\n",
        "if len(train_corpus[0]) >= 3:\n",
        "    w1, w2, w3 = train_corpus[0][:3]\n",
        "    prob = trigram_model.probability(w3, w2, w1)\n",
        "    print(f\"P({w3} | {w1}, {w2}) = {prob:.6f}\")\n",
        "else:\n",
        "    print(\"First sentence in train_corpus is too short for trigram test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZMeLotJXs8P"
      },
      "source": [
        "## Part 6: Model Evaluation - Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaxyaIi5Xs8P"
      },
      "source": [
        "### Exercise 6.1: Implement Perplexity Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "i1zKB5q1Xs8P",
        "outputId": "627fe99b-cd23-496b-eaed-8f98c98070e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing test corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:   4%|▍         | 156/3760 [00:00<00:00, 19258.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test corpus size: 100 sentences\n",
            "\n",
            "Calculating perplexities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating perplexity: 100%|██████████| 100/100 [00:00<00:00, 3183.63it/s]\n",
            "Calculating perplexity: 100%|██████████| 100/100 [00:00<00:00, 3333.18it/s]\n",
            "Calculating perplexity: 100%|██████████| 100/100 [00:00<00:00, 5048.27it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'smoothed_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1445455439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mbigram_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bigram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mtrigram_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigram_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trigram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0msmoothed_bigram_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothed_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bigram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unigram Model Perplexity: {unigram_perplexity:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'smoothed_model' is not defined"
          ]
        }
      ],
      "source": [
        "def calculate_perplexity(model, test_corpus: List[List[str]], model_type: str = \"bigram\") -> float:\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a language model.\n",
        "\n",
        "    Perplexity = exp(-1/N * sum(log P(wi | context)))\n",
        "\n",
        "    Args:\n",
        "        model: Language model with sentence_log_probability method\n",
        "        test_corpus: List of test sentences\n",
        "        model_type: 'unigram', 'bigram', or 'trigram'\n",
        "\n",
        "    Returns:\n",
        "        Perplexity score\n",
        "    \"\"\"\n",
        "    total_log_prob = 0.0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in tqdm(test_corpus, desc=\"Calculating perplexity\"):\n",
        "        if model_type == \"unigram\":\n",
        "            total_log_prob += model.sentence_log_probability(sentence)\n",
        "            total_words += len(sentence) # All words contribute to N\n",
        "        elif model_type == \"bigram\":\n",
        "            total_log_prob += model.sentence_log_probability(sentence)\n",
        "            # For bigram, the first word (<s>) does not have a previous word for a conditional probability\n",
        "            # So N is effectively len(sentence) - 1 if <s> is always the first token\n",
        "            # But sentence_log_probability already skips the first word if it calculates P(w_i | w_{i-1})\n",
        "            # So total_words should count all words that *have* a preceding word.\n",
        "            # If sentence_log_probability sums log P(wi | context) for i=1 to N-1, then N is N-1 tokens.\n",
        "            total_words += len(sentence) - 1 if len(sentence) > 1 else 0\n",
        "        elif model_type == \"trigram\":\n",
        "            total_log_prob += model.sentence_log_probability(sentence)\n",
        "            # For trigram, the first two words (<s> and w1) do not have enough preceding words\n",
        "            # So N is effectively len(sentence) - 2 if <s> is w0 and w1 is the second token\n",
        "            total_words += len(sentence) - 2 if len(sentence) > 2 else 0\n",
        "        else:\n",
        "            raise ValueError(\"Invalid model_type. Must be 'unigram', 'bigram', or 'trigram'.\")\n",
        "\n",
        "    if total_words == 0:\n",
        "        return float('inf') # Avoid division by zero, perplexity is infinite if no words counted\n",
        "\n",
        "    # Average log probability\n",
        "    avg_log_prob = total_log_prob / total_words\n",
        "\n",
        "    # Perplexity = exp(-avg_log_prob)\n",
        "    return np.exp(-avg_log_prob)\n",
        "\n",
        "# Prepare test corpus\n",
        "print(\"Preparing test corpus...\")\n",
        "test_corpus = prepare_corpus(dataset['validation'], max_examples=100)\n",
        "print(f\"Test corpus size: {len(test_corpus)} sentences\")\n",
        "\n",
        "# Calculate perplexities\n",
        "print(\"\\nCalculating perplexities...\")\n",
        "unigram_perplexity = calculate_perplexity(unigram_model, test_corpus, model_type=\"unigram\")\n",
        "bigram_perplexity = calculate_perplexity(bigram_model, test_corpus, model_type=\"bigram\")\n",
        "trigram_perplexity = calculate_perplexity(trigram_model, test_corpus, model_type=\"trigram\")\n",
        "smoothed_bigram_perplexity = calculate_perplexity(smoothed_model, test_corpus, model_type=\"bigram\")\n",
        "\n",
        "print(f\"Unigram Model Perplexity: {unigram_perplexity:.2f}\")\n",
        "print(f\"Bigram Model Perplexity: {bigram_perplexity:.2f}\")\n",
        "print(f\"Trigram Model Perplexity: {trigram_perplexity:.2f}\")\n",
        "print(f\"Smoothed Bigram Model Perplexity: {smoothed_bigram_perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmV9r3GPXs8P"
      },
      "source": [
        "## Part 7: Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-LwbuoDXs8Q"
      },
      "source": [
        "### Exercise 7.1: Implement Text Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BY7OM1mXs8Q"
      },
      "outputs": [],
      "source": [
        "class BigramGenerator:\n",
        "    \"\"\"Generate text using bigram model.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]]):\n",
        "        self.bigram_model = BigramModelSmoothed(corpus) # Use smoothed model for generation\n",
        "        self.unigram_counts = count_unigrams(corpus)\n",
        "        self.bigram_counts = count_bigrams(corpus)\n",
        "        self.vocab = build_vocabulary(corpus)\n",
        "        self.next_word_probs = self._build_next_word_distribution()\n",
        "\n",
        "    def _build_next_word_distribution(self) -> Dict[str, Tuple[List[str], List[float]]]:\n",
        "        \"\"\"Build distribution over next words for each word.\"\"\"\n",
        "        distribution = defaultdict(lambda: ([], []))\n",
        "        for prev_word in self.vocab:\n",
        "            possible_next_words = []\n",
        "            probabilities = []\n",
        "\n",
        "            # Iterate through all words in vocab to find possible next words and their probabilities\n",
        "            for next_word in self.vocab:\n",
        "                prob = self.bigram_model.probability(next_word, prev_word)\n",
        "                if prob > 0: # Only consider words with non-zero probability\n",
        "                    possible_next_words.append(next_word)\n",
        "                    probabilities.append(prob)\n",
        "\n",
        "            if possible_next_words: # Normalize probabilities if not empty\n",
        "                total_prob = sum(probabilities)\n",
        "                probabilities = [p / total_prob for p in probabilities]\n",
        "                distribution[prev_word] = (possible_next_words, probabilities)\n",
        "\n",
        "        # Ensure <s> has a distribution\n",
        "        if '<s>' not in distribution:\n",
        "            # If '<s>' doesn't have any observed next words, create a dummy distribution\n",
        "            # to allow generation to start, or handle it as an edge case.\n",
        "            # For now, let's assume '<s>' always has some observed next words in the corpus.\n",
        "            # If it truly doesn't, this part might need more robust handling.\n",
        "            pass # This case should ideally not happen if <s> is used correctly\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    def generate(self, max_length: int = 20) -> List[str]:\n",
        "        \"\"\"Generate a sentence.\"\"\"\n",
        "        sentence = ['<s>']\n",
        "        current_word = '<s>'\n",
        "\n",
        "        for _ in range(max_length - 1): # -1 because <s> is already in sentence\n",
        "            if current_word not in self.next_word_probs:\n",
        "                # If current_word has no known next word distribution, stop\n",
        "                break\n",
        "\n",
        "            possible_next_words, probabilities = self.next_word_probs[current_word]\n",
        "\n",
        "            if not possible_next_words: # No possible next words\n",
        "                break\n",
        "\n",
        "            next_word = np.random.choice(possible_next_words, p=probabilities)\n",
        "            sentence.append(next_word)\n",
        "            current_word = next_word\n",
        "\n",
        "            if current_word == '</s>':\n",
        "                break\n",
        "\n",
        "        # Ensure sentence ends with </s> if not already there and max_length not reached\n",
        "        if sentence[-1] != '</s>' and len(sentence) < max_length:\n",
        "            sentence.append('</s>')\n",
        "\n",
        "        return sentence\n",
        "\n",
        "# Test generator\n",
        "generator = BigramGenerator(train_corpus)\n",
        "print(\"Generated Sentences:\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(5): # Generate 5 sentences for testing\n",
        "    sentence = generator.generate(max_length=20)\n",
        "    print(f\"{i+1}. {' '.join(sentence)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-LSOS2IXs8Q"
      },
      "source": [
        "## Part 8: Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kejClnaXs8Q"
      },
      "source": [
        "### Exercise 8.1: Implement Add-One Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvOk6aFGXs8Q"
      },
      "outputs": [],
      "source": [
        "class BigramModelSmoothed:\n",
        "    \"\"\"Bigram model with add-one smoothing.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]]):\n",
        "        self.unigram_counts = count_unigrams(corpus)\n",
        "        self.bigram_counts = count_bigrams(corpus)\n",
        "        self.vocab = build_vocabulary(corpus)\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def probability(self, word: str, previous_word: str) -> float:\n",
        "        \"\"\"Calculate P(word | previous_word) with add-one smoothing.\n",
        "\n",
        "        Formula: P(wi | wi-1) = (C(wi-1, wi) + 1) / (C(wi-1) + V)\n",
        "        \"\"\"\n",
        "        # C(wi-1, wi) + 1\n",
        "        numerator = self.bigram_counts.get((previous_word, word), 0) + 1\n",
        "        # C(wi-1) + V\n",
        "        denominator = self.unigram_counts.get(previous_word, 0) + self.vocab_size\n",
        "\n",
        "        return numerator / denominator if denominator > 0 else 0.0\n",
        "\n",
        "    def log_probability(self, word: str, previous_word: str) -> float:\n",
        "        \"\"\"Calculate log P(word | previous_word).\"\"\"\n",
        "        prob = self.probability(word, previous_word)\n",
        "        return np.log(prob) if prob > 0 else float('-inf')\n",
        "\n",
        "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
        "        \"\"\"Calculate log P(sentence).\"\"\"\n",
        "        log_prob_sum = 0.0\n",
        "        if len(sentence) < 2:\n",
        "            return 0.0\n",
        "\n",
        "        for i in range(1, len(sentence)): # Start from the second word (index 1)\n",
        "            log_prob_sum += self.log_probability(sentence[i], sentence[i-1])\n",
        "        return log_prob_sum\n",
        "\n",
        "# Compare with unsmoothed model\n",
        "smoothed_model = BigramModelSmoothed(train_corpus)\n",
        "\n",
        "# Test on unseen bigrams\n",
        "test_unseen = [(\"<s>\", \"quantum\"), (\"the\", \"xylophone\"), (\"amazing\", \"unicorn\")]\n",
        "print(\"Comparison: Unsmoothed vs Smoothed\")\n",
        "print(\"=\" * 60)\n",
        "for w1, w2 in test_unseen:\n",
        "    unsmoothed = bigram_model.probability(w2, w1)\n",
        "    smoothed = smoothed_model.probability(w2, w1)\n",
        "    print(f\"P({w2} | {w1}):\")\n",
        "    print(f\"  Unsmoothed: {unsmoothed:.8f}\")\n",
        "    print(f\"  Smoothed:   {smoothed:.8f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0anMojUXs8Q"
      },
      "source": [
        "## Part 9: Analysis and Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD4JQAEKXs8W"
      },
      "source": [
        "### Exercise 9.1: Compare All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkpuSV6uXs8W"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a comparison table showing:\n",
        "# 1. Number of parameters (unique n-grams)\n",
        "# 2. Training perplexity\n",
        "# 3. Test perplexity\n",
        "# 4. Sample generated sentences\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate training perplexities for the models\n",
        "train_unigram_perplexity = calculate_perplexity(unigram_model, train_corpus, model_type=\"unigram\")\n",
        "train_bigram_perplexity = calculate_perplexity(bigram_model, train_corpus, model_type=\"bigram\")\n",
        "train_trigram_perplexity = calculate_perplexity(trigram_model, train_corpus, model_type=\"trigram\")\n",
        "train_smoothed_bigram_perplexity = calculate_perplexity(smoothed_model, train_corpus, model_type=\"bigram\")\n",
        "\n",
        "comparison_data = {\n",
        "    'Model': ['Unigram', 'Bigram', 'Trigram', 'Bigram (Smoothed)'],\n",
        "    'Parameters': [\n",
        "        len(unigram_counts),\n",
        "        len(bigram_counts),\n",
        "        len(trigram_counts),\n",
        "        len(bigram_counts) # Smoothed Bigram has same number of parameters as unsmoothed\n",
        "    ],\n",
        "    'Train Perplexity': [\n",
        "        train_unigram_perplexity,\n",
        "        train_bigram_perplexity,\n",
        "        train_trigram_perplexity,\n",
        "        train_smoothed_bigram_perplexity\n",
        "    ],\n",
        "    'Test Perplexity': [\n",
        "        unigram_perplexity,\n",
        "        bigram_perplexity,\n",
        "        trigram_perplexity,\n",
        "        smoothed_bigram_perplexity\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HYdHqvjXs8W"
      },
      "source": [
        "### Exercise 9.2: Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFjgcCo1Xs8W"
      },
      "outputs": [],
      "source": [
        "# TODO: Create visualizations:\n",
        "# 1. Bar chart of perplexities\n",
        "# 2. Top-20 most common words\n",
        "# 3. Top-20 most common bigrams\n",
        "\n",
        "# Example for most common words:\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Most common unigrams\n",
        "unigram_df = pd.DataFrame(unigram_counts.most_common(20), columns=['Word', 'Count'])\n",
        "axes[0].bar(unigram_df['Word'], unigram_df['Count'])\n",
        "axes[0].set_title('Top 20 Most Common Unigrams')\n",
        "axes[0].set_xlabel('Word')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 2: Most common bigrams\n",
        "bigram_df = pd.DataFrame(bigram_counts.most_common(20), columns=['Bigram', 'Count'])\n",
        "bigram_labels = [f'{w1} {w2}' for w1, w2 in bigram_df['Bigram']]\n",
        "axes[1].bar(bigram_labels, bigram_df['Count'])\n",
        "axes[1].set_title('Top 20 Most Common Bigrams')\n",
        "axes[1].set_xlabel('Bigram')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].tick_params(axis='x', rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4vpYbS0Xs8X"
      },
      "source": [
        "## Part 10: Bonus Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwDxXZj4Xs8X"
      },
      "source": [
        "### Challenge 1: Try Different Datasets\n",
        "\n",
        "Load a different dataset from HuggingFace and train your models on it.\n",
        "\n",
        "Suggestions:\n",
        "- `\"wikitext\"` (different versions)\n",
        "- `\"bookcorpus\"`\n",
        "- `\"ptb_text_only\"` (Penn Treebank)\n",
        "- `\"imdb\"` (movie reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLSsop9rXs8X"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE\n",
        "# Try loading and training on a different dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxyJFVXDXs8X"
      },
      "source": [
        "### Challenge 2: Implement Better Smoothing\n",
        "\n",
        "Implement add-k smoothing (where k < 1) or interpolation smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbjuSd8TXs8X"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6viyqY_nXs8Y"
      },
      "source": [
        "### Challenge 3: Build a Simple Autocomplete System\n",
        "\n",
        "Given a partial sentence, suggest the top-k most likely next words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Fgz7BjXs8Y"
      },
      "outputs": [],
      "source": [
        "def autocomplete(partial_sentence: str, model: BigramModel, k: int = 5) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Suggest next words for autocomplete.\n",
        "\n",
        "    Args:\n",
        "        partial_sentence: Incomplete sentence\n",
        "        model: Trained bigram model\n",
        "        k: Number of suggestions\n",
        "\n",
        "    Returns:\n",
        "        List of (word, probability) tuples\n",
        "    \"\"\"\n",
        "    # TODO: Implement this\n",
        "    pass\n",
        "\n",
        "# Test\n",
        "test_partial = \"the cat sat on the\"\n",
        "suggestions = autocomplete(test_partial, bigram_model, k=5)\n",
        "print(f\"Suggestions for: '{test_partial}'\")\n",
        "for word, prob in suggestions:\n",
        "    print(f\"  {word}: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7261ee31"
      },
      "source": [
        "# Task\n",
        "Load the 'imdb' dataset from HuggingFace, using the 'train' split for training and 'test' split for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9299137e"
      },
      "source": [
        "## Load New Dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the 'imdb' dataset from HuggingFace. We will specifically use the 'train' split for training and 'test' split for evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b91cda"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the 'imdb' dataset using `load_dataset`, then print its structure and a sample from the training split to inspect its contents as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "734051d0"
      },
      "source": [
        "print(\"Loading IMDb dataset...\")\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(imdb_dataset)\n",
        "\n",
        "# Explore the data\n",
        "print(\"\\nSample from training set:\")\n",
        "print(imdb_dataset['train'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3cebde1"
      },
      "source": [
        "## Prepare New Training Corpus\n",
        "\n",
        "### Subtask:\n",
        "Apply the `prepare_corpus` function to the 'train' split of the new 'imdb' dataset, using a smaller `max_examples` for demonstration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48e48dbd"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply the `prepare_corpus` function to the 'imdb' dataset's 'train' split with a specified `max_examples` limit, I need to call the function and then print the size and a sample of the resulting corpus for verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93fba289"
      },
      "source": [
        "print(\"Preparing IMDb training corpus...\")\n",
        "imdb_train_corpus = prepare_corpus(imdb_dataset['train'], max_examples=1000)\n",
        "\n",
        "print(f\"\\nIMDb Corpus size: {len(imdb_train_corpus)} sentences\")\n",
        "print(f\"First IMDb sentence: {imdb_train_corpus[0][:10]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e62f5862"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully prepared the IMDb training corpus. The next logical step in building an N-gram model is to build the vocabulary from this new corpus, following the original notebook's structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b943795"
      },
      "source": [
        "print(\"Building vocabulary for IMDb corpus...\")\n",
        "imdb_vocab = build_vocabulary(imdb_train_corpus)\n",
        "print(f\"IMDb Vocabulary size: {len(imdb_vocab)}\")\n",
        "print(f\"Sample IMDb words: {list(imdb_vocab)[:20]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aa3da80"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the natural progression of building an N-gram model, the next step after vocabulary creation is to count the unigrams from the new `imdb_train_corpus`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "774ee520"
      },
      "source": [
        "print(\"Counting unigrams for IMDb corpus...\")\n",
        "imdb_unigram_counts = count_unigrams(imdb_train_corpus)\n",
        "print(f\"Total IMDb unigrams: {sum(imdb_unigram_counts.values())}\")\n",
        "print(f\"Unique IMDb unigrams: {len(imdb_unigram_counts)}\")\n",
        "print(f\"\\nMost common IMDb unigrams:\")\n",
        "print(imdb_unigram_counts.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ec5520"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the natural progression of building an N-gram model, the next step after counting unigrams is to count bigrams from the new `imdb_train_corpus`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deb5cdcc"
      },
      "source": [
        "print(\"Counting bigrams for IMDb corpus...\")\n",
        "imdb_bigram_counts = count_bigrams(imdb_train_corpus)\n",
        "print(f\"Total IMDb bigrams: {sum(imdb_bigram_counts.values())}\")\n",
        "print(f\"Unique IMDb bigrams: {len(imdb_bigram_counts)}\")\n",
        "print(f\"\\nMost common IMDb bigrams:\")\n",
        "print(imdb_bigram_counts.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53e2f345"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the natural progression of building an N-gram model, the next step after counting bigrams is to count trigrams from the new `imdb_train_corpus`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c212f8cc"
      },
      "source": [
        "print(\"Counting trigrams for IMDb corpus...\")\n",
        "imdb_trigram_counts = count_trigrams(imdb_train_corpus)\n",
        "print(f\"Total IMDb trigrams: {sum(imdb_trigram_counts.values())}\")\n",
        "print(f\"Unique IMDb trigrams: {len(imdb_trigram_counts)}\")\n",
        "print(f\"\\nMost common IMDb trigrams:\")\n",
        "print(imdb_trigram_counts.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c5256b5"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that we have counted the unigrams, bigrams, and trigrams for the IMDb corpus, the next step, following the original notebook's flow, is to implement and test the Unigram Model using this new data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6281003"
      },
      "source": [
        "print(\"Building Unigram Model for IMDb corpus...\")\n",
        "imdb_unigram_model = UnigramModel(imdb_train_corpus)\n",
        "\n",
        "# Test your model\n",
        "test_words_imdb = [\"the\", \"a\", \"and\", \"<s>\", \"movie\", \"/><br\"]\n",
        "print(\"IMDb Unigram Probabilities:\")\n",
        "for word in test_words_imdb:\n",
        "    prob = imdb_unigram_model.probability(word)\n",
        "    print(f\"  P({word}) = {prob:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c613b15"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Unigram Model for the IMDb corpus has been built and tested, the next logical step, following the original notebook's structure, is to implement and test the Bigram Model using the new `imdb_train_corpus`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "109e5417"
      },
      "source": [
        "print(\"Building Bigram Model for IMDb corpus...\")\n",
        "imdb_bigram_model = BigramModel(imdb_train_corpus)\n",
        "\n",
        "# Test your model\n",
        "test_bigrams_imdb = [(\"<s>\", \"the\"), (\"the\", \"movie\"), (\"of\", \"the\"), (\"/><br\", \"/>the\")]\n",
        "print(\"IMDb Bigram Probabilities:\")\n",
        "for w1, w2 in test_bigrams_imdb:\n",
        "    prob = imdb_bigram_model.probability(w2, w1)\n",
        "    print(f\"  P({w2} | {w1}) = {prob:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a909abed"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Bigram Model for the IMDb corpus has been built and tested, the next logical step, following the original notebook's structure, is to implement and test the Trigram Model using the new `imdb_train_corpus`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2de9857f"
      },
      "source": [
        "print(\"Building Trigram Model for IMDb corpus...\")\n",
        "imdb_trigram_model = TrigramModel(imdb_train_corpus)\n",
        "\n",
        "# Test your model\n",
        "# Find a sample trigram from the corpus or create one that is likely to exist\n",
        "# For simplicity, let's pick a common trigram we found earlier, e.g., ('one', 'of', 'the')\n",
        "# or a trigram from the first sentence.\n",
        "\n",
        "# Let's try to get a valid trigram from the first sentence if it's long enough\n",
        "if len(imdb_train_corpus[0]) >= 3:\n",
        "    w1, w2, w3 = imdb_train_corpus[0][0], imdb_train_corpus[0][1], imdb_train_corpus[0][2]\n",
        "    # If the first word is '<s>', the actual context words start from index 1 and 2\n",
        "    if w1 == '<s>' and len(imdb_train_corpus[0]) >= 4:\n",
        "        w1, w2, w3 = imdb_train_corpus[0][1], imdb_train_corpus[0][2], imdb_train_corpus[0][3]\n",
        "\n",
        "    prob = imdb_trigram_model.probability(w3, w2, w1)\n",
        "    print(f\"IMDb Trigram Probability: P({w3} | {w1}, {w2}) = {prob:.6f}\")\n",
        "else:\n",
        "    print(\"First sentence in imdb_train_corpus is too short for trigram test.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1659c34e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the N-gram models (Unigram, Bigram, Trigram) have been built for the IMDb corpus, the next step is to evaluate their performance using perplexity, similar to Part 6 of the original notebook. This requires preparing a test corpus from the IMDb dataset's 'test' split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "696658d7"
      },
      "source": [
        "print(\"Preparing IMDb test corpus...\")\n",
        "imdb_test_corpus = prepare_corpus(imdb_dataset['test'], max_examples=100)\n",
        "print(f\"IMDb Test corpus size: {len(imdb_test_corpus)} sentences\")\n",
        "\n",
        "print(\"\\nCalculating IMDb model perplexities...\")\n",
        "\n",
        "imdb_unigram_perplexity = calculate_perplexity(imdb_unigram_model, imdb_test_corpus, model_type=\"unigram\")\n",
        "imdb_bigram_perplexity = calculate_perplexity(imdb_bigram_model, imdb_test_corpus, model_type=\"bigram\")\n",
        "imdb_trigram_perplexity = calculate_perplexity(imdb_trigram_model, imdb_test_corpus, model_type=\"trigram\")\n",
        "\n",
        "print(f\"IMDb Unigram Model Perplexity: {imdb_unigram_perplexity:.2f}\")\n",
        "print(f\"IMDb Bigram Model Perplexity: {imdb_bigram_perplexity:.2f}\")\n",
        "print(f\"IMDb Trigram Model Perplexity: {imdb_trigram_perplexity:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41c7a0cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'inf' perplexity for the unsmoothed unigram, bigram, and trigram models indicates that words or n-grams from the test set were not present in the training set, resulting in zero probabilities. To get meaningful perplexity scores, I will introduce add-one smoothing for unigram and trigram models, similar to the existing `BigramModelSmoothed`, and then use these smoothed models for evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea9a5601"
      },
      "source": [
        "class UnigramModelSmoothed:\n",
        "    \"\"\"Unigram language model with add-one smoothing.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]]):\n",
        "        self.unigram_counts = count_unigrams(corpus)\n",
        "        self.total_words = sum(self.unigram_counts.values())\n",
        "        self.vocab = build_vocabulary(corpus)\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def probability(self, word: str) -> float:\n",
        "        \"\"\"Calculate P(word) with add-one smoothing.\"\"\"\n",
        "        # P(word) = (C(word) + 1) / (N + V)\n",
        "        numerator = self.unigram_counts.get(word, 0) + 1\n",
        "        denominator = self.total_words + self.vocab_size\n",
        "        return numerator / denominator if denominator > 0 else 0.0\n",
        "\n",
        "    def log_probability(self, word: str) -> float:\n",
        "        \"\"\"Calculate log P(word).\"\"\"\n",
        "        prob = self.probability(word)\n",
        "        return np.log(prob) if prob > 0 else float('-inf')\n",
        "\n",
        "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
        "        \"\"\"Calculate log P(sentence).\"\"\"\n",
        "        log_prob_sum = 0.0\n",
        "        for word in sentence:\n",
        "            log_prob_sum += self.log_probability(word)\n",
        "        return log_prob_sum\n",
        "\n",
        "\n",
        "class TrigramModelSmoothed:\n",
        "    \"\"\"Trigram language model with add-one smoothing.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[List[str]]):\n",
        "        self.bigram_counts = count_bigrams(corpus)\n",
        "        self.trigram_counts = count_trigrams(corpus)\n",
        "        self.vocab = build_vocabulary(corpus) # Need vocab size for smoothing\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def probability(self, word: str, prev_word1: str, prev_word2: str) -> float:\n",
        "        \"\"\"Calculate P(word | prev_word2, prev_word1) with add-one smoothing.\"\"\"\n",
        "        # P(wi | wi-2, wi-1) = (C(wi-2, wi-1, wi) + 1) / (C(wi-2, wi-1) + V)\n",
        "        numerator = self.trigram_counts.get((prev_word2, prev_word1, word), 0) + 1\n",
        "        denominator = self.bigram_counts.get((prev_word2, prev_word1), 0) + self.vocab_size\n",
        "\n",
        "        return numerator / denominator if denominator > 0 else 0.0\n",
        "\n",
        "    def log_probability(self, word: str, prev_word1: str, prev_word2: str) -> float:\n",
        "        \"\"\"Calculate log P(word | prev_word2, prev_word1).\"\"\"\n",
        "        prob = self.probability(word, prev_word1, prev_word2)\n",
        "        return np.log(prob) if prob > 0 else float('-inf')\n",
        "\n",
        "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
        "        \"\"\"Calculate log P(sentence).\"\"\"\n",
        "        log_prob_sum = 0.0\n",
        "        if len(sentence) < 3:\n",
        "            # For very short sentences, effectively probability is 1 (log prob 0) for initial tokens\n",
        "            return 0.0\n",
        "\n",
        "        for i in range(2, len(sentence)):  # Start from the third word (index 2)\n",
        "            log_prob_sum += self.log_probability(sentence[i], sentence[i-1], sentence[i-2])\n",
        "        return log_prob_sum\n",
        "\n",
        "# Re-instantiate models with smoothing for perplexity calculation\n",
        "print(\"Building Smoothed Unigram Model for IMDb corpus...\")\n",
        "imdb_unigram_model_smoothed = UnigramModelSmoothed(imdb_train_corpus)\n",
        "print(\"Building Smoothed Bigram Model for IMDb corpus...\")\n",
        "imdb_bigram_model_smoothed = BigramModelSmoothed(imdb_train_corpus)\n",
        "print(\"Building Smoothed Trigram Model for IMDb corpus...\")\n",
        "imdb_trigram_model_smoothed = TrigramModelSmoothed(imdb_train_corpus)\n",
        "\n",
        "print(\"Preparing IMDb test corpus...\")\n",
        "imdb_test_corpus = prepare_corpus(imdb_dataset['test'], max_examples=100)\n",
        "print(f\"IMDb Test corpus size: {len(imdb_test_corpus)} sentences\")\n",
        "\n",
        "print(\"\\nCalculating IMDb model perplexities with smoothing...\")\n",
        "\n",
        "imdb_unigram_perplexity = calculate_perplexity(imdb_unigram_model_smoothed, imdb_test_corpus, model_type=\"unigram\")\n",
        "imdb_bigram_perplexity = calculate_perplexity(imdb_bigram_model_smoothed, imdb_test_corpus, model_type=\"bigram\")\n",
        "imdb_trigram_perplexity = calculate_perplexity(imdb_trigram_model_smoothed, imdb_test_corpus, model_type=\"trigram\")\n",
        "\n",
        "print(f\"IMDb Smoothed Unigram Model Perplexity: {imdb_unigram_perplexity:.2f}\")\n",
        "print(f\"IMDb Smoothed Bigram Model Perplexity: {imdb_bigram_perplexity:.2f}\")\n",
        "print(f\"IMDb Smoothed Trigram Model Perplexity: {imdb_trigram_perplexity:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c33301a"
      },
      "source": [
        "**Reasoning**:\n",
        "The request asks to add a new step to apply the `prepare_corpus` function to the 'imdb' dataset's 'train' split, using `max_examples=1000`. This will generate the `imdb_train_corpus` and print its size and a sample.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "598ee03d"
      },
      "source": [
        "print(\"Preparing IMDb training corpus...\")\n",
        "imdb_train_corpus = prepare_corpus(imdb_dataset['train'], max_examples=1000)\n",
        "\n",
        "print(f\"\\nIMDb Corpus size: {len(imdb_train_corpus)} sentences\")\n",
        "print(f\"First IMDb sentence: {imdb_train_corpus[0][:10]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01b2927a"
      },
      "source": [
        "**Reasoning**:\n",
        "The request asks to add a new step to apply the `prepare_corpus` function to the 'imdb' dataset's 'train' split, using `max_examples=1000`. This will generate the `imdb_train_corpus` and print its size and a sample.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50cd4c4f"
      },
      "source": [
        "print(\"Preparing IMDb training corpus...\")\n",
        "imdb_train_corpus = prepare_corpus(imdb_dataset['train'], max_examples=1000)\n",
        "\n",
        "print(f\"\\nIMDb Corpus size: {len(imdb_train_corpus)} sentences\")\n",
        "print(f\"First IMDb sentence: {imdb_train_corpus[0][:10]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e21a756"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully loaded and processed the IMDb corpus, and evaluated smoothed n-gram models. The next logical step, following the original notebook's structure and the user's overall goal of completing bonus challenges, is to implement the `autocomplete` function from `Exercise 7.1: Implement Text Generator` (originally part 7, now revisited in bonus challenge 3) using a smoothed bigram model. This will provide an example of text generation and prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cc5b634"
      },
      "source": [
        "def autocomplete(partial_sentence: str, model: BigramModelSmoothed, k: int = 5) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Suggest next words for autocomplete.\n",
        "\n",
        "    Args:\n",
        "        partial_sentence: Incomplete sentence\n",
        "        model: Trained bigram model (should be a smoothed one for better results)\n",
        "        k: Number of suggestions\n",
        "\n",
        "    Returns:\n",
        "        List of (word, probability) tuples\n",
        "    \"\"\"\n",
        "    # Tokenize the partial sentence, ensuring we don't add <s> and </s> mid-sentence\n",
        "    # We only care about the last word to predict the next.\n",
        "    tokens = preprocess_text(partial_sentence, add_start_end=False)\n",
        "\n",
        "    if not tokens:\n",
        "        # If the partial sentence is empty, suggest words based on <s>\n",
        "        current_word = '<s>'\n",
        "    else:\n",
        "        current_word = tokens[-1]\n",
        "\n",
        "    suggestions = []\n",
        "    # Iterate through all words in the model's vocabulary\n",
        "    for next_word in model.vocab:\n",
        "        # Exclude start and end tokens from suggestions unless specifically requested\n",
        "        if next_word in ['<s>', '</s>']:\n",
        "            continue\n",
        "\n",
        "        prob = model.probability(next_word, current_word)\n",
        "        if prob > 0:\n",
        "            suggestions.append((next_word, prob))\n",
        "\n",
        "    # Sort suggestions by probability in descending order\n",
        "    suggestions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return suggestions[:k]\n",
        "\n",
        "# Test\n",
        "# Re-initialize the BigramGenerator with the smoothed model for text generation based on IMDb data.\n",
        "# The existing BigramGenerator uses BigramModelSmoothed internally by default.\n",
        "imdb_generator = BigramGenerator(imdb_train_corpus)\n",
        "\n",
        "test_partial = \"this movie is\"\n",
        "suggestions = autocomplete(test_partial, imdb_generator.bigram_model, k=5)\n",
        "print(f\"Suggestions for: '{test_partial}'\")\n",
        "for word, prob in suggestions:\n",
        "    print(f\"  {word}: {prob:.4f}\")\n",
        "\n",
        "test_partial_2 = \"i love\"\n",
        "suggestions_2 = autocomplete(test_partial_2, imdb_generator.bigram_model, k=5)\n",
        "print(f\"\\nSuggestions for: '{test_partial_2}'\")\n",
        "for word, prob in suggestions_2:\n",
        "    print(f\"  {word}: {prob:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "808c1816"
      },
      "source": [
        "## Generate Sample Text (New Dataset)\n",
        "\n",
        "### Subtask:\n",
        "Use the `BigramGenerator` with the new smoothed bigram model to generate a few sample sentences from the new IMDb dataset to demonstrate its generative capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2fc9f60"
      },
      "source": [
        "**Reasoning**:\n",
        "To demonstrate the generative capabilities of the smoothed bigram model trained on the IMDb corpus, I will use the `imdb_generator` to produce five sample sentences and then print them, as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abb90d0b"
      },
      "source": [
        "print(\"Generated Sentences (IMDb Bigram Smoothed):\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(5): # Generate 5 sentences\n",
        "    sentence = imdb_generator.generate(max_length=20)\n",
        "    print(f\"{i+1}. {' '.join(sentence)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b41eaac"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed with a `NameError` because `imdb_generator` was not defined. This indicates that the cell initializing `imdb_generator` was either not executed or its state was lost. I need to re-initialize `imdb_generator` in this cell before attempting to use it.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}